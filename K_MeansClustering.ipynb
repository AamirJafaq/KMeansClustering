{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOqoG2URhXoKcEt3s6j3doC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AamirJafaq/KMeansClustering/blob/main/K_MeansClustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# k-Means Clustering"
      ],
      "metadata": {
        "id": "eHFpGIgUO5wk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "k-means clustering is an unsupervised learning algorithm used for data clustering, which groups unlabeled data points into groups or clusters. Among the different types of clustering algorithms such as exclusive, overlapping, hierarchical, and probabilistic, k-means is an example of an exclusive (or “hard”) clustering method. In this approach, each data point is assigned to exactly one cluster, meaning it cannot belong to multiple groups at the same time."
      ],
      "metadata": {
        "id": "2wYLCJztNdFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How does k-means work?"
      ],
      "metadata": {
        "id": "zxMHRjTowtyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The k-means clustering algorithm can be summarized in the following steps:\n",
        "1. Choose $k$ number of data points. These points will act as the initial cluster centroids.\n",
        "2. For each point, compute its distance to all K centroids and assign it to the cluster with the closest centroid. Repeating this procedure produces $k$ distinct clusters.\n",
        "3. Next, update the centroids by computing the mean of all data points assigned to each cluster.\n",
        "4. Repeat steps 2 and 3 until the centroids converge and the data point assignments remain unchanged."
      ],
      "metadata": {
        "id": "E6SxCsXjxOZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distance Metrics\n",
        "\n"
      ],
      "metadata": {
        "id": "467m8hMB23a6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An important step in clustering is to select a distance metric, which will determine how the similarity of two elements is calculated. In k-means clustering, following distrance metrics can be used:\n",
        "* Euclidean Distance\n",
        "* Minkowski Distance\n",
        "* Manhattan Distance"
      ],
      "metadata": {
        "id": "LJlj-Rec29dq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Centroid Initialization Methods"
      ],
      "metadata": {
        "id": "ss4neVRy29lq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positioning the initial centroids can be challenging, as the goal is to place them as close as possible to the true centroids’ optimal positions. Several methods for initializing centroids are described below:\\\n",
        "**Random Data Points:** \\\n",
        "In this approach, random data points are selected from the dataset and used as the initial centroids.\\\n",
        "**k-means++:** \\\n",
        "k-means++ is a smart centroid initialization method for the k-mean algorithm. The goal is to spread out the initial centroid by assigning the first centroid randomly then selecting the rest of the centroids based on the maximum squared distance. The idea is to push the centroids as far as possible from one another. \\\n",
        "Here are the simple steps to initialize centroids using k-means++:\n",
        "1. Randomly pick the first centroid $c_1$\n",
        "2. Calculate the distance between all data points and the selected centroid\n",
        "3."
      ],
      "metadata": {
        "id": "tt_I1VqL31-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to choose $k$?"
      ],
      "metadata": {
        "id": "dXTzXZjw32A9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no exact method to determine the true value of $k$. However, it can be estimated using several techniques such as cross-validation, the elbow method, information criteria, the silhouette method, and the G-means algorithm. \\\n",
        "**Elbow Method:** \\\n",
        "\n",
        "**Silhouette Method:** \\\n",
        "The silhouette coefficient measures clustering quality by evaluating how well each data point fits within its own cluster compared to other clusters. It calculates a silhouette score for each point, indicating its cohesion within its own cluster and its separation from neighboring clusters.\n",
        "$$s(i) = \\frac{b(i) - a(i)}{\\max \\{ a(i), \\, b(i) \\}}$$\n",
        "where $a(i)$ is average distance of point $i$ to all other points in the same cluster and $b(i)$ is minimum average distance of point $i$ to all points in any other cluster. \\\n",
        "A high silhouette score (close to +1) indicates that a point is well-clustered and clearly separated from other clusters, whereas a negative score suggests that the point may have been assigned to the wrong cluster. Following steps are involved in silhouette method to find an optimal value for the number of clusters $k$:\n",
        "1. Compute k-means clustering algorithm for different values of $k$.\n",
        "2. Compute the silhouette score for each data point using silhouette formula.\n",
        "3. Calculate the average silhouette score across all points for each $k$.\n",
        "4. The optimal $k$ is the one with the highest average silhouette score, since it represents well-separated and cohesive clusters."
      ],
      "metadata": {
        "id": "7sDnLg-B32Dq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering Evaluation Metrics\n",
        "\n"
      ],
      "metadata": {
        "id": "qZ_2SE34pm6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Several evaluation metrics exist for clustering, with some of the most commonly used and popular ones discussed below: \\\n",
        "**Dunn Index:** \\\n",
        "The Dunn Index is another clustering evaluation metric used to assess the quality of clusters. It measures the ratio between the minimum inter-cluster distance (how far apart the clusters are) and the maximum intra-cluster distance (the size of the largest cluster).\n",
        "$$ d = \\frac{\\min\\limits_{1 \\leq i < j \\leq k} d(c_i, c_j)}{\\max\\limits_{1 \\leq l \\leq k} \\, \\text{diam}(c_l)} $$\n",
        "where $d(c_i,c_j)$ is distance between clusters $c_i$ and $c_j$ (inter-cluster distance) and $\\text{diam}(c_l)$ is maximum distance between any two points within cluster $c_l$ (intra-cluster distance).\\\n",
        "The algorithms that create clusters with a high dunn index indicates compact, well-separated clusters and low dunn index suggests overlapping or poorly defined clusters.\\\n",
        "**Silhouette score:** \\"
      ],
      "metadata": {
        "id": "WHTU_ScG-EtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Jk3s-UJt-E8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7yUkFmDx-FGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oAaNQrcP-FJS"
      }
    }
  ]
}